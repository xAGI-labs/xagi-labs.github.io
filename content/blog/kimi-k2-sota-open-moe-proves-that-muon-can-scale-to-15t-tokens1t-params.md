---
title: "Kimi K2 - SOTA Open MoE proves that Muon can scale to 15T tokens/1T params"
date: "2025-07-11T05:44:39.000Z"
description: "**Moonshot AI** has released **Kimi K2**, a **1 trillion parameter** Mixture-of-Experts model trained on **15.5 trillion tokens** using the new **MuonClip** opt..."
original_link: "https://news.smol.ai/issues/25-07-11-kimi-k2/"
---

**Moonshot AI** has released **Kimi K2**, a **1 trillion parameter** Mixture-of-Experts model trained on **15.5 trillion tokens** using the new **MuonClip** optimizer, achieving state-of-the-art results on benchmarks like **SWE-Bench Verified (65.8%)** and **TAU2 (58.4%)**. This model is competitive with **GPT-4.1** and **Sonnet 4** on non-thinking tasks and is available under an **MIT license**. Meanwhile, **xAI** announced **Grok-4**, noted for its "LEAST censored frontier model" status and strong long-context performance but criticized for rushed post-training. **Mistral AI** updated its **Devstral 2507** models with improved performance and cost efficiency. The community is excited about the potential of the **MuonClip** optimizer, which may surpass the long-standing AdamW optimizer in machine learning.

[Read original post](https://news.smol.ai/issues/25-07-11-kimi-k2/)
