---
title: "Sora pushes SOTA"
date: "2024-02-16T11:15:03.000Z"
description: "**Discord communities** analyzed over **20 guilds**, **312 channels**, and **10550 messages** reveal intense discussions on AI developments. Key highlights incl..."
original_link: "https://news.smol.ai/issues/24-02-16-ainews-sora-pushes-sota/"
---

**Discord communities** analyzed over **20 guilds**, **312 channels**, and **10550 messages** reveal intense discussions on AI developments. Key highlights include the **Dungeon Master AI assistant** for Dungeons and Dragons using models like **H20 GPT**, GPU power supply debates involving **3090** and **3060 GPUs**, and excitement around **Google's Gemini 1.5** with its **1 million token context window** and **OpenAI's Sora** model. Challenges with **large world models (LWM)** multimodality, **GPT-assisted coding**, and **role-play model optimization** with **Yi models** and **Mixtral Instruct** were discussed. Technical issues like **model merging errors** with **MistralCasualML**, fine-tuning scripts like **AutoFineTune**, and cross-language engineering via **JSPyBridge** were also prominent. NVIDIA's **Chat with RTX** feature leveraging **retrieval-augmented generation (RAG)** on 30+ series GPUs was compared to LMStudio's support for **Mistral 7b** and **Llama 13b** models. The community is cautiously optimistic about these frontier models' applications in media and coding.

[Read original post](https://news.smol.ai/issues/24-02-16-ainews-sora-pushes-sota/)
