---
title: "NVIDIA Nemotron 3: hybrid Mamba-Transformer completely open source models from 30B to 500B"
date: "2025-12-15T05:44:39.000Z"
description: "**NVIDIA** has released **Nemotron 3 Nano**, a fully open-source hybrid Mamba-Transformer Mixture-of-Experts (MoE) model with a **30B parameter size** and a **1..."
original_link: "https://news.smol.ai/issues/25-12-15-nemotron-3/"
---

**NVIDIA** has released **Nemotron 3 Nano**, a fully open-source hybrid Mamba-Transformer Mixture-of-Experts (MoE) model with a **30B parameter size** and a **1 million token context window**. It includes open weights, training recipes, datasets, and an RL environment suite called NeMo Gym, supporting commercial use under the NVIDIA Open Model License. The model achieves state-of-the-art results on benchmarks like SWE-Bench and Artificial Analysis Intelligence Index, outperforming **Qwen3-30B A3B**. Ecosystem support is immediate with integrations into inference stacks like **vLLM**, **llama.cpp**, and **Baseten**. Upcoming larger models, Nemotron Super and Ultra, will feature NVFP4 pretraining and LatentMoE routing to optimize compute. This release marks a significant milestone for open-source American AI with comprehensive open assets and advanced hybrid architecture.

[Read original post](https://news.smol.ai/issues/25-12-15-nemotron-3/)
