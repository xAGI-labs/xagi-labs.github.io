---
title: "Companies liable for AI hallucination is Good Actually for AI Engineers"
date: "2024-02-20T00:05:26.000Z"
description: "**Air Canada** faced a legal ruling requiring it to honor refund policies communicated by its AI chatbot, setting a precedent for corporate liability in AI engi..."
original_link: "https://news.smol.ai/issues/24-02-19-ainews-companies-liable-for-ai-hallucination-is-good-actually-for-ai-engineers/"
---

**Air Canada** faced a legal ruling requiring it to honor refund policies communicated by its AI chatbot, setting a precedent for corporate liability in AI engineering accuracy. The tribunal ordered a refund of **$650.88 CAD** plus damages after the chatbot misled a customer about bereavement travel refunds. Meanwhile, AI community discussions highlighted innovations in **quantization techniques** for GPU inference, **Retrieval-Augmented Generation (RAG)** and fine-tuning of LLMs, and **CUDA** optimizations for PyTorch models. New prototype models like **Mistral-Next** and the **Large World Model (LWM)** were introduced, showcasing advances in handling large text contexts and video generation with models like **Sora**. Ethical and legal implications of AI autonomy were debated alongside challenges in dataset management. Community-driven projects such as the open-source TypeScript agent framework **bazed-af** emphasize collaborative AI development. Additionally, benchmarks like **BABILong** for up to **10M context evaluation** and tools from **karpathy** were noted.

[Read original post](https://news.smol.ai/issues/24-02-19-ainews-companies-liable-for-ai-hallucination-is-good-actually-for-ai-engineers/)
