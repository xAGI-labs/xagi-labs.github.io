---
title: "We Solved Hallucinations"
date: "2024-07-13T02:52:26.000Z"
description: "**Reddit's URL structure causes link errors in AI-generated summaries, especially with NSFW content affecting models like Claude and GPT-4.** The team fixed thi..."
original_link: "https://news.smol.ai/issues/24-07-12-ainews-we-solved-hallucinations/"
---

**Reddit's URL structure causes link errors in AI-generated summaries, especially with NSFW content affecting models like Claude and GPT-4.** The team fixed this glitch while still leveraging LLMs for summarizing Reddit content. **GPT-2 training costs have dramatically dropped to ~$672 using H100 GPUs and software improvements like CUDA and FlashAttention.** **FlashAttention-3 was released, achieving up to 740 TFLOPS on H100 GPUs, with FP8 nearing 1.2 PFLOPS, developed collaboratively by Meta, NVIDIA, Princeton, and Colfax.** Hopper GPUs enable major speedups with new hardware features. **Synthetic data may not improve vision tasks, as shown in recent research.** The **Avocado360 benchmark evaluates vision-language models' ability to detect avocados in images.** **Lynx, a hallucination detection model for LLMs, was introduced for real-world healthcare and fintech applications, trained by Patronus AI on Databricks Mosaic AI using Composer.**

[Read original post](https://news.smol.ai/issues/24-07-12-ainews-we-solved-hallucinations/)
