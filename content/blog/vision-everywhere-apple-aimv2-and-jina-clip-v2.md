---
title: "Vision Everywhere: Apple AIMv2 and Jina CLIP v2"
date: "2024-11-22T23:31:04.000Z"
description: "**Apple** released **AIMv2**, a novel vision encoder pre-trained with autoregressive objectives that achieves **89.5% accuracy on ImageNet** and integrates join..."
original_link: "https://news.smol.ai/issues/24-11-22-ainews-vision-everywhere-apple-aimv2-and-jina-clip-v2/"
---

**Apple** released **AIMv2**, a novel vision encoder pre-trained with autoregressive objectives that achieves **89.5% accuracy on ImageNet** and integrates joint visual and textual objectives. **Jina** launched **Jina CLIP v2**, a multimodal embedding model supporting **89 languages** and high-resolution images with efficient Matryoshka embeddings reducing dimensions by **94%** with minimal accuracy loss. **Allen AI** introduced **TÃ¼lu 3** models based on **Llama 3.1** with **8B and 70B** parameters, offering **2.5x faster inference** and alignment via SFT, DPO, and RLVR methods, competing with **Claude 3.5** and **Llama 3.1 70B**. These developments highlight advances in autoregressive training, vision encoders, and multilingual multimodal embeddings.

[Read original post](https://news.smol.ai/issues/24-11-22-ainews-vision-everywhere-apple-aimv2-and-jina-clip-v2/)
