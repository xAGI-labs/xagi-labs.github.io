---
title: "1/17/2024: Help crowdsource function calling datasets"
date: "2024-01-18T21:20:01.000Z"
description: "**LM Studio** updated its FAQ clarifying its **closed-source** status and perpetual freeness for personal use with no data collection. The new beta release incl..."
original_link: "https://news.smol.ai/issues/24-01-18-ainews-1172024-help-crowdsource-function-calling-datasets/"
---

**LM Studio** updated its FAQ clarifying its **closed-source** status and perpetual freeness for personal use with no data collection. The new beta release includes fixes and hints at upcoming **2-bit quantization** support. For gaming, models like **Dolphin 2.7 Mixtral 8x7B**, **MegaDolphin**, and **Dolphin 2.6 Mistral 7B DPO** with **Q4\_K\_M** quantization were recommended. Discussions highlighted that single powerful GPUs outperform multi-GPU setups due to bottlenecks, with older GPUs like Tesla P40 being cost-effective. **Microsoft's AutoGen Studio** was introduced but has issues and requires **API fees** for open-source models. Linux users are advised to use **llama.cpp** over LM Studio due to lack of headless mode. Additional tools like **LLMFarm** for iOS and various Hugging Face repositories were also mentioned. \*"LM Studio must be running to use the local inference server as there is no headless mode available"\* and \*"matching model size to GPU memory is key for performance"\* were notable points.

[Read original post](https://news.smol.ai/issues/24-01-18-ainews-1172024-help-crowdsource-function-calling-datasets/)
