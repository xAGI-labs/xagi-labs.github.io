---
title: "Reasoning Models are Near-Superhuman Coders (OpenAI IOI, Nvidia Kernels)"
date: "2025-02-14T02:42:41.000Z"
description: "**o3 model** achieved a **gold medal at the 2024 IOI** and ranks in the **99.8 percentile on Codeforces**, outperforming most humans with reinforcement learning..."
original_link: "https://news.smol.ai/issues/25-02-13-ainews-reasoning-models-are-near-superhuman-coders-openai-ioi-nvidia-kernels/"
---

**o3 model** achieved a **gold medal at the 2024 IOI** and ranks in the **99.8 percentile on Codeforces**, outperforming most humans with reinforcement learning (RL) methods proving superior to inductive bias approaches. **Nvidia's DeepSeek-R1** autonomously generates GPU kernels that surpass some expert-engineered kernels, showcasing simple yet effective AI-driven optimization. **OpenAI** updated **o1 and o3-mini** models to support file and image uploads in ChatGPT and released **DeepResearch**, a powerful research assistant based on the **o3 model with RL** for deep chain-of-thought reasoning. **Ollama** introduced **OpenThinker models** fine-tuned from **Qwen2.5**, outperforming some DeepSeek-R1 distillation models. **ElevenLabs** grew into a $3.3 billion company specializing in AI voice synthesis without open-sourcing their technology. Research highlights include **Sakana AI Labs' TAID knowledge distillation method** receiving a Spotlight at **ICLR 2025**, and **Apple's work on scaling laws for mixture-of-experts (MoEs)**. The importance of open-source AI for scientific discovery was also emphasized.

[Read original post](https://news.smol.ai/issues/25-02-13-ainews-reasoning-models-are-near-superhuman-coders-openai-ioi-nvidia-kernels/)
