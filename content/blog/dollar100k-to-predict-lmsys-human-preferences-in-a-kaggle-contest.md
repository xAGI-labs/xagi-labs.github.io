---
title: "$100k to predict LMSYS human preferences in a Kaggle contest"
date: "2024-05-03T22:09:28.000Z"
description: "**Llama 3 models** are making breakthroughs with Groq's 70B model achieving record low costs per million tokens. A new **Kaggle competition** offers a $100,000 ..."
original_link: "https://news.smol.ai/issues/24-05-03-ainews-dollar100k-to-predict-lmsys-human-preferences-in-a-kaggle-contest/"
---

**Llama 3 models** are making breakthroughs with Groq's 70B model achieving record low costs per million tokens. A new **Kaggle competition** offers a $100,000 prize to develop models predicting human preferences from a dataset of over 55,000 user-LLM conversations. Open source evaluator LLMs like **Prometheus 2** outperform proprietary models such as **GPT-4** and **Claude 3 Opus** in judgment tasks. New datasets like **WildChat1M** provide over 1 million ChatGPT interaction logs with diverse and toxic examples. Techniques like **LoRA fine-tuning** show significant performance gains, and **NVIDIA's NeMo-Aligner** toolkit enables scalable LLM alignment across hundreds of GPUs. Factuality-aware alignment methods are proposed to reduce hallucinations in LLM outputs.

[Read original post](https://news.smol.ai/issues/24-05-03-ainews-dollar100k-to-predict-lmsys-human-preferences-in-a-kaggle-contest/)
