---
title: "Apple's OpenELM beats OLMo with 50% of its dataset, using DeLighT"
date: "2024-04-26T21:32:41.000Z"
description: "**Apple** advances its AI presence with the release of **OpenELM**, its first relatively open large language model available in sizes from **270M to 3B** parame..."
original_link: "https://news.smol.ai/issues/24-04-26-ainews-apples-openelm-beats-olmo-with-50percent-of-its-dataset-using-delight/"
---

**Apple** advances its AI presence with the release of **OpenELM**, its first relatively open large language model available in sizes from **270M to 3B** parameters, featuring a novel layer-wise scaling architecture inspired by the **DeLight** paper. Meanwhile, **Meta's LLaMA 3** family pushes context length boundaries with models supporting over **160K tokens** and an **8B-Instruct model with 262K context length** released on Hugging Face, alongside performance improvements in quantized versions. A new paper on AI alignment highlights **KTO** as the best-performing method, with sensitivity to training data volume noted. In AI ethics and regulation, former **Google** CEO **Eric Schmidt** warns about the risks of open-source AI empowering bad actors and geopolitical rivals, while a U.S. proposal aims to enforce "Know Your Customer" rules to end anonymous cloud usage.

[Read original post](https://news.smol.ai/issues/24-04-26-ainews-apples-openelm-beats-olmo-with-50percent-of-its-dataset-using-delight/)
