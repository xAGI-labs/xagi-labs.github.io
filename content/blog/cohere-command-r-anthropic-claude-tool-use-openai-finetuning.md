---
title: "Cohere Command R+, Anthropic Claude Tool Use, OpenAI Finetuning"
date: "2024-04-04T22:21:15.000Z"
description: "**Cohere** launched **Command R+**, a **104B dense model** with **128k context length** focusing on **RAG**, **tool-use**, and **multilingual** capabilities acr..."
original_link: "https://news.smol.ai/issues/24-04-04-ainews-cohere-command-r-anthropic-claude-tool-use-openai-finetuning/"
---

**Cohere** launched **Command R+**, a **104B dense model** with **128k context length** focusing on **RAG**, **tool-use**, and **multilingual** capabilities across **10 key languages**. It supports **Multi-Step Tool use** and offers open weights for research. **Anthropic** introduced **tool use in beta** for **Claude**, supporting over **250 tools** with new cookbooks for practical applications. **OpenAI** enhanced its fine-tuning API with new upgrades and case studies from Indeed, SK Telecom, and Harvey, promoting DIY fine-tuning and custom model training. **Microsoft** achieved a quantum computing breakthrough with an **800x error rate improvement** and the most usable qubits to date. **Stability AI** released **Stable Audio 2.0**, improving audio generation quality and control. The **Opera browser** added local inference support for large language models like **Meta's Llama**, **Google's Gemma**, and **Vicuna**. Discussions on Reddit highlighted **Gemini's large context window**, analysis of **GPT-3.5-Turbo** model size, and a battle simulation between **Claude 3** and **ChatGPT** using local 7B models like **Mistral** and **Gemma**.

[Read original post](https://news.smol.ai/issues/24-04-04-ainews-cohere-command-r-anthropic-claude-tool-use-openai-finetuning/)
