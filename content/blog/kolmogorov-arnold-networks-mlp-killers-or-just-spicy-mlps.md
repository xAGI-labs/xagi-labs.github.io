---
title: "Kolmogorov-Arnold Networks: MLP killers or just spicy MLPs?"
date: "2024-05-07T22:47:14.000Z"
description: "**Ziming Liu**, a grad student of **Max Tegmark**, published a paper on **Kolmogorov-Arnold Networks (KANs)**, claiming they outperform **MLPs** in interpretabi..."
original_link: "https://news.smol.ai/issues/24-05-07-ainews-kolmogorov-arnold-networks-mlp-killers-or-just-spicy-mlps/"
---

**Ziming Liu**, a grad student of **Max Tegmark**, published a paper on **Kolmogorov-Arnold Networks (KANs)**, claiming they outperform **MLPs** in interpretability, inductive bias injection, function approximation accuracy, and scaling, despite being 10x slower to train but 100x more parameter efficient. KANs use learnable activation functions modeled by B-splines on edges rather than fixed activations on nodes. However, it was later shown that KANs can be mathematically rearranged back into MLPs with similar parameter counts, sparking debate on their interpretability and novelty. Meanwhile, on AI Twitter, there is speculation about a potential **GPT-5** release with mixed impressions, OpenAI's adoption of the **C2PA metadata standard** for detecting AI-generated images with high accuracy for **DALL-E 3**, and **Microsoft** training a large 500B parameter model called **MAI-1**, potentially previewed at Build conference, signaling increased competition with OpenAI. \*"OpenAI's safety testing for GPT-4.5 couldn't finish in time for Google I/O launch"\* was also noted.

[Read original post](https://news.smol.ai/issues/24-05-07-ainews-kolmogorov-arnold-networks-mlp-killers-or-just-spicy-mlps/)
