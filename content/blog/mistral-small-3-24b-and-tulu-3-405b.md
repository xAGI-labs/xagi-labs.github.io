---
title: "Mistral Small 3 24B and Tulu 3 405B"
date: "2025-01-31T00:08:47.000Z"
description: "**Mistral AI** released **Mistral Small 3**, a **24B parameter** model optimized for local inference with low latency and **81% accuracy on MMLU**, competing wi..."
original_link: "https://news.smol.ai/issues/25-01-30-ainews-mistral-small-3-24b-and-tulu-3-405b/"
---

**Mistral AI** released **Mistral Small 3**, a **24B parameter** model optimized for local inference with low latency and **81% accuracy on MMLU**, competing with **Llama 3.3 70B**, **Qwen-2.5 32B**, and **GPT4o-mini**. **AI2** released **TÃ¼lu 3 405B**, a large finetuned model of **Llama 3** using Reinforcement Learning from Verifiable Rewards (RVLR), competitive with **DeepSeek v3**. **Sakana AI** launched **TinySwallow-1.5B**, a Japanese language model using **TAID** for on-device use. **Alibaba\_Qwen** released **Qwen 2.5 Max**, trained on **20 trillion tokens**, with performance comparable to **DeepSeek V3**, **Claude 3.5 Sonnet**, and **Gemini 1.5 Pro**, and updated API pricing. These releases highlight advances in open models, efficient inference, and reinforcement learning techniques.

[Read original post](https://news.smol.ai/issues/25-01-30-ainews-mistral-small-3-24b-and-tulu-3-405b/)
