---
title: "Mistral 3: Mistral Large 3 + Ministral 3B/8B/14B open weights models"
date: "2025-12-02T05:44:39.000Z"
description: "**Mistral** has launched the **Mistral 3 family** including **Ministral 3** models (3B/8B/14B) and **Mistral Large 3**, a sparse MoE model with **675B total par..."
original_link: "https://news.smol.ai/issues/25-12-02-mistral-3/"
---

**Mistral** has launched the **Mistral 3 family** including **Ministral 3** models (3B/8B/14B) and **Mistral Large 3**, a sparse MoE model with **675B total parameters** and **256k context window**, all under an Apache 2.0 open license. Early benchmarks rank Mistral Large 3 at **#6 among open models** with strong coding performance. The launch includes broad ecosystem support such as vLLM, llama.cpp, Ollama, and LM Studio integrations. Meanwhile, **Anthropic** acquired the open-source **Bun** runtime to accelerate **Claude Code**, which reportedly reached a **$1B run-rate in ~6 months**. Anthropic also announced discounted **Claude** plans for nonprofits and shared insights on AI's impact on work internally.

[Read original post](https://news.smol.ai/issues/25-12-02-mistral-3/)
