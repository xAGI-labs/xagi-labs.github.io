---
title: "1/11/2024: Mixing Experts vs Merging Models"
date: "2024-01-12T18:49:15.000Z"
description: "**18 guilds**, **277 channels**, and **1342 messages** were analyzed with an estimated reading time saved of **187 minutes**. The community switched to **GPT-4 ..."
original_link: "https://news.smol.ai/issues/24-01-12-ainews-1112024-mixing-experts-vs-merging-models/"
---

**18 guilds**, **277 channels**, and **1342 messages** were analyzed with an estimated reading time saved of **187 minutes**. The community switched to **GPT-4 turbo** and discussed the rise of **Mixture of Experts (MoE) models** like **Mixtral**, **DeepSeekMOE**, and **Phixtral**. Model merging techniques, including naive linear interpolation and "frankenmerges" by **SOLAR** and **Goliath**, are driving new performance gains on open leaderboards. Discussions in the **Nous Research AI Discord** covered topics such as AI playgrounds supporting prompt and RAG parameters, security concerns about third-party cloud usage, debates on Discord bots and TOS, skepticism about **Teenage Engineering's** cloud LLM, and performance differences between **GPT-4 0613** and **GPT-4 turbo**. The community also explored fine-tuning strategies involving **DPO**, **LoRA**, and safetensors, integration of RAG with API calls, semantic differences between MoE and dense LLMs, and data frameworks like **llama index** and **SciPhi-AI's synthesizer**. Issues with anomalous characters in fine-tuning were also raised.

[Read original post](https://news.smol.ai/issues/24-01-12-ainews-1112024-mixing-experts-vs-merging-models/)
