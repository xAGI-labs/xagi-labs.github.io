---
title: "RWKV \"Eagle\" v5: Your move, Mamba"
date: "2024-01-30T01:20:56.000Z"
description: "**RWKV v5 Eagle** was released with better-than-**mistral-7b** evaluation results, trading some English performance for multilingual capabilities. The mysteriou..."
original_link: "https://news.smol.ai/issues/24-01-29-ainews-rwkv-eagle-v5-your-move-mamba/"
---

**RWKV v5 Eagle** was released with better-than-**mistral-7b** evaluation results, trading some English performance for multilingual capabilities. The mysterious **miqu-1-70b** model sparked debate about its origins, possibly a leak or distillation of **Mistral Medium** or a fine-tuned **Llama 2**. Discussions highlighted fine-tuning techniques, including the effectiveness of **1,000 high-quality prompts** over larger mixed-quality datasets, and tools like **Deepspeed**, **Axolotl**, and **QLoRA**. The **Nous Research AI** community emphasized the impact of **Rotary Position Embedding (RoPE) theta settings** on LLM extrapolation, improving models like **Mistral Instruct v0.2**. Speed improvements in **Mistral Tuna** kernels reduced token processing costs, enhancing efficiency. The launch of **Eagle 7B** with 7.52B parameters showcased strong multilingual performance, surpassing other 7B class models.

[Read original post](https://news.smol.ai/issues/24-01-29-ainews-rwkv-eagle-v5-your-move-mamba/)
