---
title: "MiniMax M2 230BA10B — 8% of Claude Sonnet's price, ~2x faster, new SOTA open model"
date: "2025-10-27T05:44:39.000Z"
description: "**MiniMax M2**, an open-weight sparse MoE model by **Hailuo AI**, launches with **≈200–230B parameters** and **10B active parameters**, offering strong performa..."
original_link: "https://news.smol.ai/issues/25-10-27-minimax-m2/"
---

**MiniMax M2**, an open-weight sparse MoE model by **Hailuo AI**, launches with **≈200–230B parameters** and **10B active parameters**, offering strong performance near frontier closed models and ranking #5 overall on the Artificial Analysis Intelligence Index v3.0. It supports coding and agent tasks, is licensed under **MIT**, and is available via API at competitive pricing. The architecture uses **full attention**, **QK-Norm**, **GQA**, partial RoPE, and sigmoid routing, with day-0 support in **vLLM** and deployment on platforms like Hugging Face and Baseten. Despite verbosity and no tech report, it marks a significant win for open models.

[Read original post](https://news.smol.ai/issues/25-10-27-minimax-m2/)
