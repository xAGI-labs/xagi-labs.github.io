---
title: "Gemini 2.5 Pro/Flash GA, 2.5 Flash-Lite in Preview"
date: "2025-06-17T05:44:39.000Z"
description: "**Gemini 2.5** models are now generally available, including the new **Gemini 2.5 Flash-Lite**, **Flash**, **Pro**, and **Ultra** variants, featuring sparse **M..."
original_link: "https://news.smol.ai/issues/25-06-17-gemini-2-5/"
---

**Gemini 2.5** models are now generally available, including the new **Gemini 2.5 Flash-Lite**, **Flash**, **Pro**, and **Ultra** variants, featuring sparse **Mixture-of-Experts (MoE)** transformers with native multimodal support. A detailed 30-page tech report highlights impressive long-horizon planning demonstrated by **Gemini Plays Pokemon**. The **LiveCodeBench-Pro** benchmark reveals frontier LLMs struggle with hard coding problems, while **Moonshot AI** open-sourced **Kimi-Dev-72B**, achieving state-of-the-art results on **SWE-bench Verified**. Smaller specialized models like **Nanonets-OCR-s**, **II-Medical-8B-1706**, and **Jan-nano** show competitive performance, emphasizing that bigger models are not always better. **DeepSeek-r1** ties for #1 in WebDev Arena, and **MiniMax-M1** sets new standards in long-context reasoning. **Kling AI** demonstrated video generation capabilities.

[Read original post](https://news.smol.ai/issues/25-06-17-gemini-2-5/)
