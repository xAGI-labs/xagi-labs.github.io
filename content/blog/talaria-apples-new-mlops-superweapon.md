---
title: "Talaria: Apple's new MLOps Superweapon"
date: "2024-06-11T06:41:05.000Z"
description: "**Apple Intelligence** introduces a small (~3B parameters) on-device model and a larger server model running on Apple Silicon with Private Cloud Compute, aiming..."
original_link: "https://news.smol.ai/issues/24-06-10-ainews-talaria-apples-new-mlops-superweapon/"
---

**Apple Intelligence** introduces a small (~3B parameters) on-device model and a larger server model running on Apple Silicon with Private Cloud Compute, aiming to surpass **Google Gemma**, **Mistral Mixtral**, **Microsoft Phi**, and **Mosaic DBRX**. The on-device model features a novel lossless quantization strategy using mixed 2-bit and 4-bit LoRA adapters averaging 3.5 bits-per-weight, enabling dynamic adapter hot-swapping and efficient memory management. Apple credits the **Talaria** tool for optimizing quantization and model latency, achieving about 0.6 ms time-to-first-token latency and 30 tokens per second generation rate on iPhone 15 Pro. Apple focuses on an "adapter for everything" strategy with initial deployment on SiriKit and App Intents. Performance benchmarks rely on human graders, emphasizing consumer-level adequacy over academic dominance. The Apple ML blog also mentions an Xcode code-focused model and a diffusion model for Genmoji.

[Read original post](https://news.smol.ai/issues/24-06-10-ainews-talaria-apples-new-mlops-superweapon/)
