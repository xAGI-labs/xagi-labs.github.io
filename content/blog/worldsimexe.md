---
title: "World_sim.exe"
date: "2024-03-20T00:46:48.000Z"
description: "**NVIDIA** announced **Project GR00T**, a foundation model for humanoid robot learning using multimodal instructions, built on their tech stack including Isaac ..."
original_link: "https://news.smol.ai/issues/24-03-19-ainews-worldsimexe/"
---

**NVIDIA** announced **Project GR00T**, a foundation model for humanoid robot learning using multimodal instructions, built on their tech stack including Isaac Lab, OSMO, and Jetson Thor. They revealed the **DGX Grace-Blackwell GB200** with over **1 exaflop** compute, capable of training **GPT-4 1.8T parameters** in 90 days on 2000 Blackwells. Jensen Huang confirmed GPT-4 has **1.8 trillion parameters**. The new **GB200 GPU** supports float4/6 precision with ~3 bits per parameter and achieves **40,000 TFLOPs** on fp4 with 2x sparsity. Open source highlights include the release of **Grok-1**, a **340B parameter** model, and **Stability AI's SV3D**, an open-source text-to-video generation solution. **Nous Research** collaborated on implementing Steering Vectors in Llama.CPP. In Retrieval Augmented Generation (RAG), a new **5.5-hour tutorial** builds a pipeline using open-source HF models, and **LangChain** released a video on query routing and announced integration with **NVIDIA NIM** for GPU-optimized LLM inference. Prominent opinions include **Yann LeCun** distinguishing language from other cognitive abilities, **Sam Altman** predicting AGI arrival in 6 years with a leap from GPT-4 to GPT-5 comparable to GPT-3 to GPT-4, and discussions on the philosophical status of LLMs like Claude. There is also advice against training models from scratch for most companies.

[Read original post](https://news.smol.ai/issues/24-03-19-ainews-worldsimexe/)
