---
title: "Mini, Nemo, Turbo, Lite - Smol models go brrr (GPT4o-mini version)"
date: "2024-07-19T00:13:31.000Z"
description: "**OpenAI** launched the **GPT-4o Mini**, a cost-efficient small model priced at **$0.15 per million input tokens** and **$0.60 per million output tokens**, aimi..."
original_link: "https://news.smol.ai/issues/24-07-18-ainews-mini-nemo-turbo-lite-smol-models-go-brrr-gpt4o-mini-version/"
---

**OpenAI** launched the **GPT-4o Mini**, a cost-efficient small model priced at **$0.15 per million input tokens** and **$0.60 per million output tokens**, aiming to replace **GPT-3.5 Turbo** with enhanced intelligence but some performance limitations. **DeepSeek** open-sourced **DeepSeek-V2-0628**, topping the LMSYS Chatbot Arena Leaderboard and emphasizing their commitment to contributing to the AI ecosystem. **Mistral AI** and **NVIDIA** released the **Mistral NeMo**, a **12B parameter** multilingual model with a record **128k token context window** under an **Apache 2.0 license**, sparking debates on benchmarking accuracy against models like **Meta Llama 8B**. Research breakthroughs include the **TextGrad** framework for optimizing compound AI systems via textual feedback differentiation and the **STORM** system improving article writing by **25%** through simulating diverse perspectives and addressing source bias. Developer tooling trends highlight **LangChain**'s evolving context-aware reasoning applications and the **Modular** ecosystem's new official GPU support, including discussions on **Mojo** and **Keras 3.0** integration.

[Read original post](https://news.smol.ai/issues/24-07-18-ainews-mini-nemo-turbo-lite-smol-models-go-brrr-gpt4o-mini-version/)
