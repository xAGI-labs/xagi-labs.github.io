---
title: "Reflection 70B, by Matt from IT Department"
date: "2024-09-07T01:17:07.000Z"
description: "**Reflection Tuning** technique has been used by a two-person team from **Hyperwrite** and **Glaive** to finetune **llama-3.1-70b**, showing strong performance ..."
original_link: "https://news.smol.ai/issues/24-09-06-ainews-reflection-70b-by-matt-from-it-department/"
---

**Reflection Tuning** technique has been used by a two-person team from **Hyperwrite** and **Glaive** to finetune **llama-3.1-70b**, showing strong performance improvements with minimal synthetic data. The approach builds on the concept of adding \`thinking\` and \`reflection\` steps to outputs, related to the **Chain of Thought** method. Despite some criticisms like contamination concerns, worse coding performance, and reliance on system prompts, the model has received positive reception and comparisons to **claude-3.5-sonnet**. The work highlights efficient instruction tuning and synthetic data generation for large models.

[Read original post](https://news.smol.ai/issues/24-09-06-ainews-reflection-70b-by-matt-from-it-department/)
