---
title: "Grok-1 in Bio"
date: "2024-03-19T00:07:45.000Z"
description: "**Grok-1**, a **314B parameter Mixture-of-Experts (MoE) model** from **xAI**, has been released under an Apache 2.0 license, sparking discussions on its archite..."
original_link: "https://news.smol.ai/issues/24-03-18-ainews-grok-1-in-bio/"
---

**Grok-1**, a **314B parameter Mixture-of-Experts (MoE) model** from **xAI**, has been released under an Apache 2.0 license, sparking discussions on its architecture, finetuning challenges, and performance compared to models like **Mixtral** and **Miqu 70B**. Despite its size, its **MMLU benchmark performance** is currently unimpressive, with expectations that **Grok-2** will be more competitive. The model's weights and code are publicly available, encouraging community experimentation. **Sam Altman** highlighted the growing importance of compute resources, while **Grok's** potential deployment on **Groq hardware** was noted as a possible game-changer. Meanwhile, **Anthropic's Claude** continues to attract attention for its "spiritual" interaction experience and consistent ethical framework. The release also inspired memes and humor within the AI community.

[Read original post](https://news.smol.ai/issues/24-03-18-ainews-grok-1-in-bio/)
