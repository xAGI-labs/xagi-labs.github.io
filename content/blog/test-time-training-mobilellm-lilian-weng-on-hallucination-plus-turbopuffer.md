---
title: "Test-Time Training, MobileLLM, Lilian Weng on Hallucination (Plus: Turbopuffer)"
date: "2024-07-10T05:57:13.000Z"
description: "**Lilian Weng** released a comprehensive literature review on **hallucination detection** and **anti-hallucination methods** including techniques like Factualit..."
original_link: "https://news.smol.ai/issues/24-07-09-ainews-test-time-training-mobilellm-lilian-weng-on-hallucination-plus-turbopuffer/"
---

**Lilian Weng** released a comprehensive literature review on **hallucination detection** and **anti-hallucination methods** including techniques like FactualityPrompt, SelfCheckGPT, and WebGPT. **Facebook AI Research (FAIR)** published **MobileLLM**, a sub-billion parameter on-device language model architecture achieving performance comparable to **llama-2-7b** with innovations like thin and deep models and shared weights. A new **RNN-based LLM architecture** with expressive hidden states was introduced, replacing attention mechanisms and scaling better than Mamba and Transformer models for long-context modeling. Additionally, **Tsinghua University** open sourced **CodeGeeX4-ALL-9B**, a multilingual code generation model excelling in code assistance.

[Read original post](https://news.smol.ai/issues/24-07-09-ainews-test-time-training-mobilellm-lilian-weng-on-hallucination-plus-turbopuffer/)
