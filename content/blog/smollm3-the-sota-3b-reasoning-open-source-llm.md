---
title: "SmolLM3: the SOTA 3B reasoning open source LLM"
date: "2025-07-08T05:44:39.000Z"
description: "**HuggingFace** released **SmolLM3-3B**, a fully open-source small reasoning model with open pretraining code and data, marking a high point in open source mode..."
original_link: "https://news.smol.ai/issues/25-07-08-smollm3/"
---

**HuggingFace** released **SmolLM3-3B**, a fully open-source small reasoning model with open pretraining code and data, marking a high point in open source models until **Olmo 3** arrives. **Grok 4** was launched with mixed reactions, while concerns about **Claude 4** nerfs and an imminent **Claude 4.1** surfaced. **Gemini Nano** is now shipping in **Chrome 137+**, enabling local LLM access for **3.7 billion** users. **Tencent** introduced **Hunyuan-A13B**, an 80B parameter model with a 256K context window running on a single **H200** GPU. The **Gemini API** added a batch mode with 50% discounts on **2.5 models**. **MatFormer Lab** launched tools for custom-sized **Gemma 3n** models. Open source OCR models like **Nanonets-OCR-s** and **ChatDOC/OCRFlux-3B** derived from **Qwen2.5-VL-3B** were highlighted, with licensing discussions involving **Alibaba**.

[Read original post](https://news.smol.ai/issues/25-07-08-smollm3/)
