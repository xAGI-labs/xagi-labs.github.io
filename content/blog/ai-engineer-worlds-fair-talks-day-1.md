---
title: "AI Engineer World's Fair Talks Day 1"
date: "2025-06-04T05:44:39.000Z"
description: "**Mistral** launched a new **Code** project, and **Cursor** released version **1.0**. **Anthropic** improved **Claude Code** plans, while **ChatGPT** announced ..."
original_link: "https://news.smol.ai/issues/25-06-04-not-much/"
---

**Mistral** launched a new **Code** project, and **Cursor** released version **1.0**. **Anthropic** improved **Claude Code** plans, while **ChatGPT** announced expanded connections. The day was dominated by **AIE** keynotes and tracks including **GraphRAG**, **RecSys**, and **Tiny Teams**. On Reddit, **Google** open-sourced the **DeepSearch** stack for building AI agents with **Gemini 2.5** and **LangGraph**, enabling flexible agent architectures and integration with local LLMs like **Gemma**. A new **Meta** paper analyzed language model memorization, showing GPT-style transformers store about **3.5â€“4 bits/parameter** and exploring the transition from memorization to generalization, with implications for **Mixture-of-Experts** models and quantization effects.

[Read original post](https://news.smol.ai/issues/25-06-04-not-much/)
