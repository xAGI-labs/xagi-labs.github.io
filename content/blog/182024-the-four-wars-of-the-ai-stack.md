---
title: "1/8/2024: The Four Wars of the AI Stack"
date: "2024-01-09T07:39:51.000Z"
description: "The **Nous Research AI Discord** discussions highlighted several key topics including the use of **DINO**, **CLIP**, and **CNNs** in the **Obsidian Project**. A..."
original_link: "https://news.smol.ai/issues/24-01-08-ainews-182024-the-four-wars-of-the-ai-stack/"
---

The **Nous Research AI Discord** discussions highlighted several key topics including the use of **DINO**, **CLIP**, and **CNNs** in the **Obsidian Project**. A research paper on distributed models like **DistAttention** and **DistKV-LLM** was shared to address cloud-based **LLM** service challenges. Another paper titled 'Self-Extend LLM Context Window Without Tuning' argued that existing **LLMs** can handle long contexts inherently. The community also discussed AI models like **Mixtral**, favored for its **32k context window**, and compared it with **Mistral** and **Marcoroni**. Other topics included hierarchical embeddings, agentic retrieval-augmented generation (**RAG**), synthetic data for fine-tuning, and the application of **LLMs** in the oil & gas industry. The launch of the **AgentSearch-V1** dataset with one billion embedding vectors was also announced. The discussions covered **mixture-of-experts (MoE)** implementations and the performance of smaller models.

[Read original post](https://news.smol.ai/issues/24-01-08-ainews-182024-the-four-wars-of-the-ai-stack/)
