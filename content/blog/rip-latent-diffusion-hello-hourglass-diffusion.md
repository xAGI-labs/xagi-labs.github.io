---
title: "RIP Latent Diffusion, Hello Hourglass Diffusion"
date: "2024-01-24T01:38:15.000Z"
description: "**Katherine Crowson** from **Stable Diffusion** introduces a hierarchical pure transformer backbone for diffusion-based image generation that efficiently scales..."
original_link: "https://news.smol.ai/issues/24-01-23-ainews-rip-latent-diffusion-hello-hourglass-diffusion/"
---

**Katherine Crowson** from **Stable Diffusion** introduces a hierarchical pure transformer backbone for diffusion-based image generation that efficiently scales to megapixel resolutions with under 600 million parameters, improving upon the original ~900M parameter model. This architecture processes local and global image phenomena separately, enhancing efficiency and resolution without latent steps. Additionally, Meta's Self Rewarding LM paper has inspired **lucidrains** to begin an implementation. Discord summaries highlight GPT-4's robustness against quantification tricks, discussions on open-source GPT-0 alternatives, challenges in DPO training on limited VRAM with suggestions like QLoRA and rmsprop, and efforts to improve roleplay model consistency through fine-tuning and merging. Philosophical debates on AI sentience and GPT-4 customization for markdown and translation tasks were also noted.

[Read original post](https://news.smol.ai/issues/24-01-23-ainews-rip-latent-diffusion-hello-hourglass-diffusion/)
