---
title: "Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata"
date: "2024-06-14T21:06:38.000Z"
description: "**NVIDIA** has scaled up its **Nemotron-4** model from **15B** to a massive **340B** dense model, trained on **9T tokens**, achieving performance comparable to ..."
original_link: "https://news.smol.ai/issues/24-06-14-ainews-nemotron-4-340b-nvidias-new-large-open-models-built-on-syndata-great-for-syndata/"
---

**NVIDIA** has scaled up its **Nemotron-4** model from **15B** to a massive **340B** dense model, trained on **9T tokens**, achieving performance comparable to **GPT-4**. The model alignment process uses over **98% synthetic data**, with only about **20K human-annotated samples** for fine-tuning and reward model training. The synthetic data generation pipeline is open-sourced, including synthetic prompts and preference data generation. The base and instruct versions outperform **Mixtral** and **Llama 3**, while the reward model ranks better than **Gemini 1.5**, **Cohere**, and **GPT-4o**. Other notable models include **Mamba-2-Hybrid 8B**, which is up to **8x faster** than Transformers and excels on long-context tasks, **Samba-3.8B-instruct** for infinite context length with linear complexity, **Dolphin-2.9.3** tiny models optimized for low-resource devices, and **Faro Yi 9B DPO** with a **200K context window** running efficiently on **16GB VRAM**. The Mixture-of-Agents technique boosts open-source LLMs beyond GPT-4 Omni on AlpacaEval 2.0.

[Read original post](https://news.smol.ai/issues/24-06-14-ainews-nemotron-4-340b-nvidias-new-large-open-models-built-on-syndata-great-for-syndata/)
