---
title: "12/9/2023: The Mixtral Rush"
date: "2023-12-09T23:30:00.000Z"
description: "**Mixtral's weights** were released without code, prompting the **Disco Research community** and **Fireworks AI** to implement it rapidly. Despite efforts, no s..."
original_link: "https://news.smol.ai/issues/23-12-09-ainews-1292023-the-mixtral-rush/"
---

**Mixtral's weights** were released without code, prompting the **Disco Research community** and **Fireworks AI** to implement it rapidly. Despite efforts, no significant benchmark improvements were reported, limiting its usefulness for local LLM usage but marking progress for the **small models community**. Discussions in the DiscoResearch Discord covered **Mixtral's performance** compared to models like **Hermes 2.5** and **Hermes 2**, with evaluations on benchmarks such as **winogrande**, **truthfulqa\_mc2**, and **arc\_challenge**. Technical topics included GPU requirements, multi-GPU setups, and quantization via **GPTQ**. Benchmarking strategies like grammar-based evaluation, chain of thought (CoT), and min\_p sampling were explored, alongside model sampling techniques like Min P and Top P to enhance response stability and creativity. Users also discussed GPTs' learning limitations and the adaptability of models under varying conditions, emphasizing min\_p sampling's role in enabling higher temperature settings for creativity.

[Read original post](https://news.smol.ai/issues/23-12-09-ainews-1292023-the-mixtral-rush/)
