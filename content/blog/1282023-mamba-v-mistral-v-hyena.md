---
title: "12/8/2023 - Mamba v Mistral v Hyena"
date: "2023-12-08T22:40:04.000Z"
description: "Three new AI models are highlighted: **Mistral's 8x7B MoE model (Mixtral)**, **Mamba models** up to 3B by Together, and **StripedHyena 7B**, a competitive subqu..."
original_link: "https://news.smol.ai/issues/23-12-08-ainews-1282023-mamba-v-mistral-v-hyena/"
---

Three new AI models are highlighted: **Mistral's 8x7B MoE model (Mixtral)**, **Mamba models** up to 3B by Together, and **StripedHyena 7B**, a competitive subquadratic attention model from Stanford's Hazy Research. Discussions on **Anthropic's Claude 2.1** focus on its prompting technique and alignment challenges. The **Gemini AI** from Google is noted as potentially superior to **GPT-4**. The community also explores **Dreambooth** for image training and shares resources like the **DialogRPT-human-vs-machine** model on Hugging Face. Deployment challenges for large language models, including CPU performance and GPU requirements, are discussed with references to **Falcon 180B** and transformer batching techniques. User engagement includes meme sharing and humor.

[Read original post](https://news.smol.ai/issues/23-12-08-ainews-1282023-mamba-v-mistral-v-hyena/)
