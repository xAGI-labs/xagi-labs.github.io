---
title: "BitNet was a lie?"
date: "2024-11-13T01:36:06.000Z"
description: "**Scaling laws for quantization** have been modified by a group led by Chris Re, analyzing over **465 pretraining runs** and finding benefits plateau at FP6 pre..."
original_link: "https://news.smol.ai/issues/24-11-12-ainews-bitnet-was-a-lie/"
---

**Scaling laws for quantization** have been modified by a group led by Chris Re, analyzing over **465 pretraining runs** and finding benefits plateau at FP6 precision. Lead author **Tanishq Kumar** highlights that longer training and more data increase sensitivity to quantization, explaining challenges with models like **Llama-3**. **Tim Dettmers**, author of QLoRA, warns that the era of efficiency gains from low-precision quantization is ending, signaling a shift from scaling to optimizing existing resources. Additionally, **Alibaba** announced **Qwen 2.5-Coder-32B-Instruct**, which matches or surpasses **GPT-4o** on coding benchmarks, and open-source initiatives like **DeepEval** for LLM testing are gaining traction.

[Read original post](https://news.smol.ai/issues/24-11-12-ainews-bitnet-was-a-lie/)
