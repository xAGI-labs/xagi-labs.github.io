---
title: "Mixtral 8x22B Instruct sparks efficiency memes"
date: "2024-04-17T21:02:34.000Z"
description: "**Mistral** released an instruct-tuned version of their **Mixtral 8x22B** model, notable for using only **39B active parameters** during inference, outperformin..."
original_link: "https://news.smol.ai/issues/24-04-17-ainews-mixtral-8x22b-instruct-sparks-efficiency-memes/"
---

**Mistral** released an instruct-tuned version of their **Mixtral 8x22B** model, notable for using only **39B active parameters** during inference, outperforming larger models and supporting **5 languages** with **64k context window** and math/code capabilities. The model is available on **Hugging Face** under an **Apache 2.0 license** for local use. **Google** plans to invest over **$100 billion** in AI, with other giants like **Microsoft**, **Intel**, and **SoftBank** also making large investments. The UK criminalized non-consensual deepfake porn, raising enforcement debates. A former **Nvidia** employee claims Nvidia's AI chip lead is unmatchable this decade. AI companions could become a **$1 billion** market. AI has surpassed humans on several basic tasks but lags on complex ones. **Zyphra** introduced **Zamba**, a novel 7B parameter hybrid model outperforming **LLaMA-2 7B** and **OLMo-7B** with less training data, trained on 128 H100 GPUs over 30 days. **GroundX** API advances retrieval-augmented generation accuracy.

[Read original post](https://news.smol.ai/issues/24-04-17-ainews-mixtral-8x22b-instruct-sparks-efficiency-memes/)
