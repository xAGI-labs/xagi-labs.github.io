---
title: "Jamba: Mixture of Architectures dethrones Mixtral"
date: "2024-03-28T23:43:23.000Z"
description: "**AI21 labs** released **Jamba**, a **52B parameter MoE model** with **256K context length** and open weights under Apache 2.0 license, optimized for single A10..."
original_link: "https://news.smol.ai/issues/24-03-28-ainews-jamba-mixture-of-architectures-dethrones-mixtral/"
---

**AI21 labs** released **Jamba**, a **52B parameter MoE model** with **256K context length** and open weights under Apache 2.0 license, optimized for single A100 GPU performance. It features a unique blocks-and-layers architecture combining transformer and MoE layers, competing with models like **Mixtral**. Meanwhile, **Databricks** introduced **DBRX**, a **36B active parameter MoE model** trained on **12T tokens**, noted as a new standard for open LLMs. In image generation, advancements include **Animatediff** for video-quality image generation and **FastSD CPU v1.0.0 beta 28** enabling ultra-fast image generation on CPUs. Other innovations involve style-content separation using **B-LoRA** and improvements in high-resolution image upscaling with **SUPIR**.

[Read original post](https://news.smol.ai/issues/24-03-28-ainews-jamba-mixture-of-architectures-dethrones-mixtral/)
