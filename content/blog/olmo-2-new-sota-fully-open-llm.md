---
title: "OLMo 2 - new SOTA Fully Open LLM"
date: "2024-11-27T05:17:18.000Z"
description: "**AI2** has updated **OLMo-2** to roughly **Llama 3.1 8B** equivalent, training with **5T tokens** and using learning rate annealing and new high-quality data (..."
original_link: "https://news.smol.ai/issues/24-11-26-ainews-olmo-2-new-sota-fully-open-llm/"
---

**AI2** has updated **OLMo-2** to roughly **Llama 3.1 8B** equivalent, training with **5T tokens** and using learning rate annealing and new high-quality data (Dolmino). They credit **TÃ¼lu 3** and its "Reinforcement Learning with Verifiable Rewards" approach. On Reddit, **Qwen2.5-72B instruct** model shows near lossless performance with **AutoRound 4-bit quantization**, available on **HuggingFace** in 4-bit and 2-bit versions, with discussions on **MMLU** benchmark and quantization-aware training. **HuggingFace** released **SmolVLM**, a **2B parameter** vision-language model running efficiently on consumer GPUs, supporting fine-tuning on Google Colab and demonstrating strong OCR capabilities with adjustable resolution and quantization options.

[Read original post](https://news.smol.ai/issues/24-11-26-ainews-olmo-2-new-sota-fully-open-llm/)
