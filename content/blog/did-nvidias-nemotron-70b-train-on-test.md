---
title: "Did Nvidia's Nemotron 70B train on test?"
date: "2024-10-17T00:44:43.000Z"
description: "**NVIDIA's Nemotron-70B** model has drawn scrutiny despite strong benchmark performances on **Arena Hard**, **AlpacaEval**, and **MT-Bench**, with some standard..."
original_link: "https://news.smol.ai/issues/24-10-16-ainews-did-nvidias-nemotron-70b-train-on-test/"
---

**NVIDIA's Nemotron-70B** model has drawn scrutiny despite strong benchmark performances on **Arena Hard**, **AlpacaEval**, and **MT-Bench**, with some standard benchmarks like **GPQA** and **MMLU Pro** showing no improvement over the base **Llama-3.1-70B**. The new **HelpSteer2-Preference dataset** improves some benchmarks with minimal losses elsewhere. Meanwhile, **Mistral** released **Ministral 3B and 8B** models featuring **128k context length** and outperforming **Llama-3.1** and **GPT-4o** on various benchmarks under the **Mistral Commercial License**. **NVIDIA's Nemotron 70B** also surpasses **GPT-4o** and **Claude-3.5-Sonnet** on key benchmarks using **RLHF (REINFORCE)** training. Additionally, **Zep** introduced **Graphiti**, an open-source temporal knowledge graph memory layer for AI agents, built on **Neo4j**.

[Read original post](https://news.smol.ai/issues/24-10-16-ainews-did-nvidias-nemotron-70b-train-on-test/)
