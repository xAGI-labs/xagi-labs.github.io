---
title: "12/30/2023: Mega List of all LLMs"
date: "2023-12-31T10:23:31.000Z"
description: "**Stella Biderman**'s tracking list of **LLMs** is highlighted, with resources shared for browsing. The **Nous Research AI** Discord discussed the **Local Atten..."
original_link: "https://news.smol.ai/issues/23-12-31-ainews-12302023-mega-list-of-all-llms/"
---

**Stella Biderman**'s tracking list of **LLMs** is highlighted, with resources shared for browsing. The **Nous Research AI** Discord discussed the **Local Attention Flax** module focusing on computational complexity, debating linear vs quadratic complexity and proposing chunking as a solution. Benchmark logs for various LLMs including **Deita v1.0** with its **SFT+DPO** training method were shared. Discussions covered model merging, graded modal types, function calling in AI models, and data contamination issues in **Mixtral**. Community insights were sought on **Amazon Titan Text Express** and **Amazon Titan Text Lite** LLMs, including a unique training strategy involving bad datasets. Several GitHub repositories and projects like **DRUGS**, **MathPile**, **CL-FoMo**, and **SplaTAM** were referenced for performance and data quality evaluations.

[Read original post](https://news.smol.ai/issues/23-12-31-ainews-12302023-mega-list-of-all-llms/)
