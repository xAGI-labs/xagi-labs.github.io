---
title: "s{imple|table|calable} Consistency Models"
date: "2024-10-25T02:36:02.000Z"
description: "**Model distillation** significantly accelerates diffusion models, enabling near real-time image generation with only 1-4 sampling steps, as seen in **BlinkShot..."
original_link: "https://news.smol.ai/issues/24-10-24-ainews-simpleortableorcalable-consistency-models/"
---

**Model distillation** significantly accelerates diffusion models, enabling near real-time image generation with only 1-4 sampling steps, as seen in **BlinkShot** and **Flux Schnell**. Research led by **Yang Song** introduced **simplified continuous-time consistency models (sCMs)**, achieving under 10% FID difference in just 2 steps and scaling up to **1.5B parameters** for higher quality. On AI hardware, **Tesla** is deploying a **50k H100 cluster** potentially capable of completing **GPT-4** training in under three weeks, while **Cerebras Systems** set a new inference speed record on **Llama 3.1 70B** with their wafer-scale AI chips. **Stability AI** released **Stable Diffusion 3.5** and its Turbo variant, and **Cohere** launched new multilingual models supporting **23 languages** with state-of-the-art performance. **LangChain** also announced ecosystem updates.

[Read original post](https://news.smol.ai/issues/24-10-24-ainews-simpleortableorcalable-consistency-models/)
