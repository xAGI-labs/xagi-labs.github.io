---
title: "Gemini launches context caching... or does it?"
date: "2024-06-18T21:26:50.000Z"
description: "**Nvidia's Nemotron** ranks #1 open model on LMsys and #11 overall, surpassing **Llama-3-70b**. **Meta AI** released **Chameleon 7B/34B** models after further p..."
original_link: "https://news.smol.ai/issues/24-06-18-ainews-gemini-launches-context-caching-or-does-it/"
---

**Nvidia's Nemotron** ranks #1 open model on LMsys and #11 overall, surpassing **Llama-3-70b**. **Meta AI** released **Chameleon 7B/34B** models after further post-training. **Google's Gemini** introduced context caching, offering a cost-efficient middle ground between RAG and finetuning, with a minimum input token count of 33k and no upper limit on cache duration. **DeepSeek** launched **DeepSeek-Coder-V2**, a 236B parameter model outperforming **GPT-4 Turbo**, **Claude-3-Opus**, and **Gemini-1.5-Pro** in coding tasks, supporting 338 programming languages and extending context length to 128K. It was trained on 6 trillion tokens using the **Group Relative Policy Optimization (GRPO)** algorithm and is available on Hugging Face with a commercial license. These developments highlight advances in model performance, context caching, and large-scale coding models.

[Read original post](https://news.smol.ai/issues/24-06-18-ainews-gemini-launches-context-caching-or-does-it/)
