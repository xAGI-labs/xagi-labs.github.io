---
title: "DeepSeek V3.1: 840B token continued pretrain, beating Claude 4 Sonnet at 11% of its cost"
date: "2025-08-20T05:44:39.000Z"
description: "**DeepSeek** released **DeepSeek V3.1**, a quietly rolled out open model with an **128K context window** and improvements in **token efficiency**, coding, and a..."
original_link: "https://news.smol.ai/issues/25-08-20-deepseekv31/"
---

**DeepSeek** released **DeepSeek V3.1**, a quietly rolled out open model with an **128K context window** and improvements in **token efficiency**, coding, and agentic benchmarks. **ByteDance** launched the permissive **Seed-OSS 36B** model on Hugging Face, noted for long-context and reasoning capabilities. **Zhipu AI** introduced **ComputerRL**, a reinforcement learning framework for computer-use agents, achieving strong benchmark results. In developer tooling, **GitHub Copilot** expanded globally, **Microsoft VS Code** integrated **Gemini 2.5 Pro** and updated **GPT-5** agent prompts, and **Anthropic** launched **Claude Code** seats with spend controls. Open-source fine-tuning advances include **Together AI** adding SFT for **gpt-oss-120B/20B** and **Baseten** enabling multinode 120B training with Truss CLI. The community noted mixed performance and ongoing post-training adjustments for DeepSeek V3.1.

[Read original post](https://news.smol.ai/issues/25-08-20-deepseekv31/)
