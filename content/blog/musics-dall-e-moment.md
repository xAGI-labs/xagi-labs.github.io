---
title: "Music's Dall-E moment"
date: "2024-04-10T22:07:48.000Z"
description: "**Google's Griffin architecture** outperforms transformers with faster inference and lower memory usage on long contexts. **Command R+** climbs to 6th place on ..."
original_link: "https://news.smol.ai/issues/24-04-10-ainews-musics-dall-e-moment/"
---

**Google's Griffin architecture** outperforms transformers with faster inference and lower memory usage on long contexts. **Command R+** climbs to 6th place on the LMSYS Chatbot Arena leaderboard, surpassing **GPT-4-0613** and **GPT-4-0314**. **Mistral AI** releases an open-source **8x22B model** with a 64K context window and around 130B total parameters. **Google** open-sources **CodeGemma** models with pre-quantized 4-bit versions for faster downloads. **Ella weights** enhance Stable Diffusion 1.5 with LLM for semantic alignment. **Unsloth** enables 4x larger context windows and 80% memory reduction for finetuning. **Andrej Karpathy** releases LLMs implemented in pure C for potential performance gains. **Command R+** runs in realtime on M2 Max MacBook using iMat q1 quantization. **Cohere's Command R** model offers low API costs and strong leaderboard performance. **Gemini 1.5** impresses with audio capabilities recognizing speech tone and speaker identification from audio clips.

[Read original post](https://news.smol.ai/issues/24-04-10-ainews-musics-dall-e-moment/)
