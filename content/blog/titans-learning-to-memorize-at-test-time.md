---
title: "Titans: Learning to Memorize at Test Time"
date: "2025-01-16T07:58:41.000Z"
description: "**Google** released a new paper on \"Neural Memory\" integrating persistent memory directly into transformer architectures at test time, showing promising long-..."
original_link: "https://news.smol.ai/issues/25-01-15-ainews-titans-learning-to-memorize-at-test-time/"
---

**Google** released a new paper on "Neural Memory" integrating persistent memory directly into transformer architectures at test time, showing promising long-context utilization. **MiniMax-01** by @omarsar0 features a **4 million token context window** with **456B parameters** and **32 experts**, outperforming **GPT-4o** and **Claude-3.5-Sonnet**. **InternLM3-8B-Instruct** is an open-source model trained on **4 trillion tokens** with state-of-the-art results. **TransformerÂ²** introduces self-adaptive LLMs that dynamically adjust weights for continuous adaptation. Advances in AI security highlight the need for **agent authentication**, **prompt injection** defenses, and **zero-trust architectures**. Tools like **Micro Diffusion** enable budget-friendly diffusion model training, while **LeagueGraph** and **Agent Recipes** support open-source social media agents.

[Read original post](https://news.smol.ai/issues/25-01-15-ainews-titans-learning-to-memorize-at-test-time/)
