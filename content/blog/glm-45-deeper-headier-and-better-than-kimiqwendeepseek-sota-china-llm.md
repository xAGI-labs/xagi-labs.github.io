---
title: "GLM-4.5: Deeper, Headier, & better than Kimi/Qwen/DeepSeek (SOTA China LLM?)"
date: "2025-07-28T05:44:39.000Z"
description: "**Z.ai** (Zhipu AI) released the **GLM-4.5-355B-A32B** and **GLM-4.5-Air-106B-A12B** open weights models, claiming state-of-the-art performance competitive with..."
original_link: "https://news.smol.ai/issues/25-07-28-glm-45/"
---

**Z.ai** (Zhipu AI) released the **GLM-4.5-355B-A32B** and **GLM-4.5-Air-106B-A12B** open weights models, claiming state-of-the-art performance competitive with **Claude 4 Opus**, **Grok 4**, and OpenAI's **o3**. These models emphasize token efficiency and efficient reinforcement learning training validated by the Muon optimizer. **Alibaba Qwen** introduced **Group Sequence Policy Optimization (GSPO)**, a new reinforcement learning algorithm powering the **Qwen3** model suite, integrated into Hugging Face's TRL library. Speculation surrounds mystery models "summit" and "zenith" as potential **GPT-5** variants based on **GPT-4.1** architecture. **Qwen3-Coder** shows strong coding benchmark results, rivaling **Claude Sonnet 4** and **Kimi K2**. The rise of powerful Chinese open-source models like **GLM-4.5**, **Wan-2.2**, and **Qwen3 Coder** contrasts with a slowdown from Western labs such as **OpenAI**.

[Read original post](https://news.smol.ai/issues/25-07-28-glm-45/)
