---
title: "Llama 4's Controversial Weekend Release"
date: "2025-04-08T01:55:40.000Z"
description: "**Meta** released **Llama 4**, featuring two new medium-size MoE open models and a promised 2 Trillion parameter \"behemoth\" model, aiming to be the largest op..."
original_link: "https://news.smol.ai/issues/25-04-07-ainews-llama-4s-controversial-weekend-release/"
---

**Meta** released **Llama 4**, featuring two new medium-size MoE open models and a promised 2 Trillion parameter "behemoth" model, aiming to be the largest open model ever. The release included advanced training techniques like Chameleon-like early fusion with MetaCLIP, interleaved chunked attention without RoPE, native FP8 training, and training on up to 40 trillion tokens. Despite the hype, the release faced criticism for lack of transparency compared to Llama 3, implementation issues, and poor performance on some benchmarks. Meta leadership, including **Ahmad Al Dahle**, denied allegations of training on test sets. The smallest Scout model at 109B parameters is too large for consumer GPUs, and the claimed 10 million token context is disputed. The community response has been mixed, with some praising the openness and others pointing out discrepancies and quality concerns.

[Read original post](https://news.smol.ai/issues/25-04-07-ainews-llama-4s-controversial-weekend-release/)
