---
title: "Mistral Large 2 + RIP Mistral 7B, 8x7B, 8x22B"
date: "2024-07-24T23:44:31.000Z"
description: "**Mistral Large 2** introduces **123B parameters** with **Open Weights** under a Research License, focusing on **code generation**, **math performance**, and a ..."
original_link: "https://news.smol.ai/issues/24-07-24-ainews-mistral-large-2-rip-mistral-7b-8x7b-8x22b/"
---

**Mistral Large 2** introduces **123B parameters** with **Open Weights** under a Research License, focusing on **code generation**, **math performance**, and a massive **128k context window**, improving over Mistral Large 1's 32k context. It claims better **function calling** capabilities than **GPT-4o** and enhanced reasoning. Meanwhile, **Meta** officially released **Llama-3.1** models including **Llama-3.1-70B** and **Llama-3.1-8B** with detailed pre-training and post-training insights. The **Llama-3.1 8B** model's 128k context performance was found underwhelming compared to **Mistral Nemo** and **Yi 34B 200K**. Mistral is deprecating older Apache open-source models, focusing on Large 2 and **Mistral Nemo 12B**. The news also highlights community discussions and benchmarking comparisons.

[Read original post](https://news.smol.ai/issues/24-07-24-ainews-mistral-large-2-rip-mistral-7b-8x7b-8x22b/)
