---
title: "Miqu confirmed to be an early Mistral-medium checkpoint"
date: "2024-01-31T23:15:13.000Z"
description: "**Miqu**, an open access model, scores **74 on MMLU** and **84.5 on EQ-Bench**, sparking debates about its performance compared to **Mistral Medium**. The **CEO..."
original_link: "https://news.smol.ai/issues/24-01-31-ainews-miqu-confirmed-to-be-an-early-mistral-medium-checkpoint/"
---

**Miqu**, an open access model, scores **74 on MMLU** and **84.5 on EQ-Bench**, sparking debates about its performance compared to **Mistral Medium**. The **CEO of Mistral** confirmed these results. Discussions in the **TheBloke Discord** highlight **Miqu's** superiority in instruction-following and sampling methods like dynatemp and min-p. Developers also explore browser preferences and Discord UI themes. Role-playing with models like **BagelMistery Tour v2** and **Psyfighter v2** is popular, alongside technical talks on **fp16 quantization** of **Miqu-1-70b**. Training and fine-tuning tips for models like **Unsloth** and **Mistral 7B** are shared. In the **Nous Research AI Discord**, the **Activation Beacon** method is discussed for extending LLM context length from 4K to 400K tokens. **SQLCoder-70B**, fine-tuned on **CodeLlama-70B**, leads in text-to-SQL generation and is available on Hugging Face. The **Miqu model** also impresses with an **83.5 EQ-Bench score**, fueling speculation about its capabilities.

[Read original post](https://news.smol.ai/issues/24-01-31-ainews-miqu-confirmed-to-be-an-early-mistral-medium-checkpoint/)
