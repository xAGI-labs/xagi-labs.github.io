---
title: "Gemini 2.0 Flash GA, with new Flash Lite, 2.0 Pro, and Flash Thinking"
date: "2025-02-06T02:00:20.000Z"
description: "**Google DeepMind** officially launched **Gemini 2.0** models including **Flash**, **Flash-Lite**, and **Pro Experimental**, with **Gemini 2.0 Flash** outperfor..."
original_link: "https://news.smol.ai/issues/25-02-05-ainews-gemini-20-flash-ga-with-new-flash-lite-20-pro-and-flash-thinking/"
---

**Google DeepMind** officially launched **Gemini 2.0** models including **Flash**, **Flash-Lite**, and **Pro Experimental**, with **Gemini 2.0 Flash** outperforming **Gemini 1.5 Pro** while being **12x cheaper** and supporting **multimodal input** and a **1 million token context window**. **Andrej Karpathy** released a **3h31m** video deep dive into **large language models**, covering **pretraining**, **fine-tuning**, and **reinforcement learning** with examples like **GPT-2** and **Llama 3.1**. A free course on **Transformer architecture** was introduced by **Jay Alammar**, **Maarten Gr**, and **Andrew Ng**, focusing on **tokenizers**, **embeddings**, and **mixture-of-expert models**. **DeepSeek-R1** reached **1.2 million downloads** on **Hugging Face** with a detailed **36-page technical report**. **Anthropic** increased rewards to **$10K** and **$20K** for their jailbreak challenge, while **BlueRaven** extension was updated to hide Twitter metrics for unbiased engagement.

[Read original post](https://news.smol.ai/issues/25-02-05-ainews-gemini-20-flash-ga-with-new-flash-lite-20-pro-and-flash-thinking/)
