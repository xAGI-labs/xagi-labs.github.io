---
title: "12/11/2023: Mixtral beats GPT3.5 and Llama2-70B"
date: "2023-12-11T20:11:07.000Z"
description: "**Mistral AI** announced the **Mixtral 8x7B** model featuring a Sparse Mixture of Experts (SMoE) architecture, sparking discussions on its potential to rival **..."
original_link: "https://news.smol.ai/issues/23-12-11-ainews-12112023-mixtral-beats-gpt35-and-llama2-70b/"
---

**Mistral AI** announced the **Mixtral 8x7B** model featuring a Sparse Mixture of Experts (SMoE) architecture, sparking discussions on its potential to rival **GPT-4**. The community debated GPU hardware options for training and fine-tuning transformer models, including **RTX 4070s**, **A4500**, **RTX 3090s with nvlink**, and **A100 GPUs**. Interest was expressed in fine-tuning Mixtral and generating quantized versions, alongside curating high-quality coding datasets. Resources shared include a YouTube video on open-source model deployment, an Arxiv paper, GitHub repositories, and a blog post on Mixture-of-Experts. Discussions also touched on potential open-source releases of **GPT-3.5 Turbo** and **llama-3**, and running **OpenHermes 2.5** on Mac M3 Pro with VRAM considerations.

[Read original post](https://news.smol.ai/issues/23-12-11-ainews-12112023-mixtral-beats-gpt35-and-llama2-70b/)
