---
title: "Gemma 2: The Open Model for Everyone"
date: "2024-06-28T06:21:39.000Z"
description: "**Gemma 2**, a **27B** parameter model from **google-deepmind**, was released with innovations like 1:1 local-global attention alternation and logit soft-cappin..."
original_link: "https://news.smol.ai/issues/24-06-27-ainews-gemma-2-the-open-model-for-everyone/"
---

**Gemma 2**, a **27B** parameter model from **google-deepmind**, was released with innovations like 1:1 local-global attention alternation and logit soft-capping, leveraging **knowledge distillation** to train smaller models on over 50Ã— the compute-optimal token quantity. The model supports multilingual and multimodal capabilities, with fine-tuning success on over 200 Indic language variants. The **Open LLM Leaderboard** highlights **alibaba's Qwen 72B** as the top model, with **mistral-ai's Mixtral-8x22B-Instruct** also ranking highly. **Anthropic** launched **Claude 3.5 Sonnet**, improving intelligence at mid-tier cost and speed. Research on eliminating matrix multiplication in LLMs promises significant memory savings without performance loss. \*Kathleen Kenealy\* and \*Daniel Han\* provided insights on Gemma 2's tokenizer and attention scaling respectively.

[Read original post](https://news.smol.ai/issues/24-06-27-ainews-gemma-2-the-open-model-for-everyone/)
