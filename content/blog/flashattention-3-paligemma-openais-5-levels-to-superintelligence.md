---
title: "FlashAttention 3, PaliGemma, OpenAI's 5 Levels to Superintelligence"
date: "2024-07-12T09:31:43.000Z"
description: "**FlashAttention-3** introduces fast and accurate attention optimized for **H100 GPUs**, advancing native **FP8 training**. **PaliGemma**, a versatile **3B Visi..."
original_link: "https://news.smol.ai/issues/24-07-12-ainews-flashattention-3-paligemma-openais-5-levels-to-superintelligence/"
---

**FlashAttention-3** introduces fast and accurate attention optimized for **H100 GPUs**, advancing native **FP8 training**. **PaliGemma**, a versatile **3B Vision-Language Model (VLM)** combining a SigLIP-So400m ViT encoder with the **Gemma-2B** language model, emphasizes a prefix-LM architecture for improved image-query interaction. **OpenAI** reveals a framework on levels of superintelligence, signaling progress toward Level 2 and highlighting internal safety disagreements. On Reddit, **NuminaMath 7B**, fine-tuned from **DeepSeekMath-7B**, wins the AI Math Olympiad by solving 29 problems using iterative supervised fine-tuning and tool-integrated reasoning. Open-source LLMs like **CodeLlama-34b** and **WizardCoder-Python-34B-V1.0** are closing the coding performance gap with closed models such as **ChatGPT-3.5**.

[Read original post](https://news.smol.ai/issues/24-07-12-ainews-flashattention-3-paligemma-openais-5-levels-to-superintelligence/)
