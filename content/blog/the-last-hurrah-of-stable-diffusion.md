---
title: "The Last Hurrah of Stable Diffusion?"
date: "2024-06-12T22:08:29.000Z"
description: "**Stability AI** launched **Stable Diffusion 3 Medium** with models ranging from **450M to 8B parameters**, featuring the MMDiT architecture and T5 text encoder..."
original_link: "https://news.smol.ai/issues/24-06-12-ainews-the-last-hurrah-of-stable-diffusion/"
---

**Stability AI** launched **Stable Diffusion 3 Medium** with models ranging from **450M to 8B parameters**, featuring the MMDiT architecture and T5 text encoder for image text rendering. The community has shown mixed reactions following the departure of key researchers like Emad Mostaque. On AI models, **Llama 3 8B Instruct** shows strong evaluation correlation with **GPT-4**, while **Qwen 2 Instruct** surpasses Llama 3 on MMLU benchmarks. The **Mixture of Agents (MoA)** framework outperforms GPT-4o on AlpacaEval 2.0. Techniques like **Spectrum** and **QLoRA** enable efficient fine-tuning with less VRAM. Research on **grokking** reveals transformers can transition from memorization to generalization through extended training. Benchmark initiatives include the **$1M ARC Prize Challenge** for AGI progress and **LiveBench**, a live LLM benchmark to prevent dataset contamination. The **Character Codex Dataset** offers open data on over **15,000 characters** for RAG and synthetic data. The **MLX 0.2** tool enhances LLM experience on Apple Silicon Macs with improved UI and faster retrieval-augmented generation.

[Read original post](https://news.smol.ai/issues/24-06-12-ainews-the-last-hurrah-of-stable-diffusion/)
